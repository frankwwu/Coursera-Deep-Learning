# Week 2: Optimization Algorithms

Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate decay scheduling to speed up your models.

11 videos (Total 92 min), 2 readings, 3 quizzes

## 11 videos

* Mini-batch Gradient Descent 11 m
* Understanding Mini-batch Gradient Descent 11 m
* Exponentially Weighted Averages 5 m
* Understanding Exponentially Weighted Averages 9 m
* Bias Correction in Exponentially Weighted Averages 4 m
* Gradient Descent with Momentum  9 m
* RMSprop 7 m
* Adam Optimization Algorithm 7 m
* Learning Rate Decay 6 m
* The Problem of Local Optima 5 m
* Yuanqing Lin Interview 13 m

## 2 readings

* Clarification about Upcoming Adam Optimization Video 1 m
* Clarification about Learning Rate Decay Video 1 m

## 1 practice exercise

* Optimization Algorithms 50 m